{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee39d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07bdd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/sam/Desktop/samlab/codes/DataScience/datasets/Churn_Modelling.csv')\n",
    "\n",
    "from sklearn import datasets\n",
    "df = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64c6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing\n",
    "# df.drop(['Surname'], axis =1, inplace = True) # remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868b42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df.iloc[:,:-1]\n",
    "# y = df.iloc[:,-1]\n",
    "# x = pd.get_dummies(x, drop_first = True)\n",
    "\n",
    "# another data\n",
    "x = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5164b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910cc0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "----------\n",
      "[[21  0  0]\n",
      " [ 0 29  4]\n",
      " [ 0  1 20]]\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Logregg = LogisticRegression(random_state = 42).fit(x_train, y_train)\n",
    "prediction = Logregg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "acc = accuracy_score(prediction, y_test)\n",
    "mat = confusion_matrix(prediction, y_test)\n",
    "print(acc)\n",
    "print('----------')\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0f99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d2746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "----------\n",
      "[[21  0  0]\n",
      " [ 0 29  1]\n",
      " [ 0  2 22]]\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Logregg = DecisionTreeClassifier(random_state = 42)\n",
    "Logregg.fit(x_train, y_train)\n",
    "prediction = Logregg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "acc = accuracy_score(prediction, y_test)\n",
    "mat = confusion_matrix(y_test, prediction)\n",
    "print(acc)\n",
    "print('----------')\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f8a5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-8-9a24a4f24ba4>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-9a24a4f24ba4>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print('---\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Logregg = RandomForestClassifier(random_state = 21)\n",
    "Logregg.fit(x_train, y_train)\n",
    "prediction = Logregg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "acc = accuracy_score(prediction, y_test)\n",
    "mat = confusion_matrix(prediction, y_test)\n",
    "print(acc)\n",
    "print('---\n",
    "      --r-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79190142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966707cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "Logregg = KNeighborsClassifier()\n",
    "Logregg.fit(x_train, y_train)\n",
    "prediction = Logregg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "acc = accuracy_score(prediction, y_test)\n",
    "mat = confusion_matrix(prediction, y_test)\n",
    "print(acc)\n",
    "print('-----r-----')\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe255633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff01ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "Logregg = SGDClassifier(loss=\"hinge\", penalty=\"l1\", max_iter=19\n",
    "                    )\n",
    "Logregg.fit(x_train, y_train)\n",
    "prediction = Logregg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "acc = accuracy_score(prediction, y_test)\n",
    "mat = confusion_matrix(prediction, y_test)\n",
    "print(acc)\n",
    "print('-----r-----')\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "75/75 [==============================] - 0s 1ms/sample - loss: 0.6472 - acc: 0.2667\n",
      "Epoch 2/1000\n",
      "75/75 [==============================] - 0s 201us/sample - loss: 0.6247 - acc: 0.2667\n",
      "Epoch 3/1000\n",
      "75/75 [==============================] - 0s 224us/sample - loss: 0.6019 - acc: 0.2667\n",
      "Epoch 4/1000\n",
      "75/75 [==============================] - 0s 230us/sample - loss: 0.5808 - acc: 0.2667\n",
      "Epoch 5/1000\n",
      "75/75 [==============================] - 0s 211us/sample - loss: 0.5621 - acc: 0.2667\n",
      "Epoch 6/1000\n",
      "75/75 [==============================] - 0s 218us/sample - loss: 0.5462 - acc: 0.2667\n",
      "Epoch 7/1000\n",
      "75/75 [==============================] - 0s 232us/sample - loss: 0.5298 - acc: 0.2667\n",
      "Epoch 8/1000\n",
      "75/75 [==============================] - 0s 214us/sample - loss: 0.5141 - acc: 0.2667\n",
      "Epoch 9/1000\n",
      "75/75 [==============================] - 0s 226us/sample - loss: 0.5000 - acc: 0.2667\n",
      "Epoch 10/1000\n",
      "75/75 [==============================] - 0s 226us/sample - loss: 0.4864 - acc: 0.2667\n",
      "Epoch 11/1000\n",
      "75/75 [==============================] - 0s 219us/sample - loss: 0.4731 - acc: 0.2667\n",
      "Epoch 12/1000\n",
      "75/75 [==============================] - 0s 257us/sample - loss: 0.4597 - acc: 0.2667\n",
      "Epoch 13/1000\n",
      "75/75 [==============================] - 0s 214us/sample - loss: 0.4471 - acc: 0.2667\n",
      "Epoch 14/1000\n",
      "75/75 [==============================] - 0s 193us/sample - loss: 0.4365 - acc: 0.2667\n",
      "Epoch 15/1000\n",
      "75/75 [==============================] - 0s 215us/sample - loss: 0.4245 - acc: 0.2667\n",
      "Epoch 16/1000\n",
      "75/75 [==============================] - 0s 272us/sample - loss: 0.4138 - acc: 0.2667\n",
      "Epoch 17/1000\n",
      "75/75 [==============================] - 0s 233us/sample - loss: 0.4048 - acc: 0.2667\n",
      "Epoch 18/1000\n",
      "75/75 [==============================] - 0s 222us/sample - loss: 0.3926 - acc: 0.2667\n",
      "Epoch 19/1000\n",
      "75/75 [==============================] - 0s 229us/sample - loss: 0.3826 - acc: 0.2667\n",
      "Epoch 20/1000\n",
      "75/75 [==============================] - 0s 282us/sample - loss: 0.3742 - acc: 0.2667\n",
      "Epoch 21/1000\n",
      "75/75 [==============================] - 0s 262us/sample - loss: 0.3649 - acc: 0.2667\n",
      "Epoch 22/1000\n",
      "75/75 [==============================] - 0s 151us/sample - loss: 0.3554 - acc: 0.2667\n",
      "Epoch 23/1000\n",
      "75/75 [==============================] - 0s 242us/sample - loss: 0.3477 - acc: 0.2667\n",
      "Epoch 24/1000\n",
      "75/75 [==============================] - 0s 250us/sample - loss: 0.3404 - acc: 0.2667\n",
      "Epoch 25/1000\n",
      "75/75 [==============================] - 0s 251us/sample - loss: 0.3338 - acc: 0.2667\n",
      "Epoch 26/1000\n",
      "75/75 [==============================] - 0s 267us/sample - loss: 0.3268 - acc: 0.2667\n",
      "Epoch 27/1000\n",
      "75/75 [==============================] - 0s 268us/sample - loss: 0.3221 - acc: 0.2667\n",
      "Epoch 28/1000\n",
      "75/75 [==============================] - 0s 238us/sample - loss: 0.3154 - acc: 0.2667\n",
      "Epoch 29/1000\n",
      "75/75 [==============================] - 0s 210us/sample - loss: 0.3108 - acc: 0.2667\n",
      "Epoch 30/1000\n",
      "75/75 [==============================] - 0s 254us/sample - loss: 0.3052 - acc: 0.2667\n",
      "Epoch 31/1000\n",
      "75/75 [==============================] - 0s 291us/sample - loss: 0.3005 - acc: 0.2667\n",
      "Epoch 32/1000\n",
      "75/75 [==============================] - 0s 285us/sample - loss: 0.2957 - acc: 0.2667\n",
      "Epoch 33/1000\n",
      "75/75 [==============================] - 0s 218us/sample - loss: 0.2912 - acc: 0.2667\n",
      "Epoch 34/1000\n",
      "75/75 [==============================] - 0s 235us/sample - loss: 0.2867 - acc: 0.2667\n",
      "Epoch 35/1000\n",
      "75/75 [==============================] - 0s 258us/sample - loss: 0.2830 - acc: 0.2667\n",
      "Epoch 36/1000\n",
      "75/75 [==============================] - 0s 224us/sample - loss: 0.2780 - acc: 0.2667\n",
      "Epoch 37/1000\n",
      "75/75 [==============================] - 0s 203us/sample - loss: 0.2750 - acc: 0.2667\n",
      "Epoch 38/1000\n",
      "75/75 [==============================] - 0s 234us/sample - loss: 0.2714 - acc: 0.2667\n",
      "Epoch 39/1000\n",
      "75/75 [==============================] - 0s 225us/sample - loss: 0.2678 - acc: 0.2667\n",
      "Epoch 40/1000\n",
      "75/75 [==============================] - 0s 267us/sample - loss: 0.2644 - acc: 0.2667\n",
      "Epoch 41/1000\n",
      "75/75 [==============================] - 0s 286us/sample - loss: 0.2605 - acc: 0.2667\n",
      "Epoch 42/1000\n",
      "75/75 [==============================] - 0s 232us/sample - loss: 0.2573 - acc: 0.2667\n",
      "Epoch 43/1000\n",
      "75/75 [==============================] - 0s 243us/sample - loss: 0.2543 - acc: 0.2667\n",
      "Epoch 44/1000\n",
      "75/75 [==============================] - 0s 208us/sample - loss: 0.2510 - acc: 0.2667\n",
      "Epoch 45/1000\n",
      "75/75 [==============================] - 0s 235us/sample - loss: 0.2485 - acc: 0.2667\n",
      "Epoch 46/1000\n",
      "75/75 [==============================] - 0s 232us/sample - loss: 0.2475 - acc: 0.2667\n",
      "Epoch 47/1000\n",
      "75/75 [==============================] - 0s 262us/sample - loss: 0.2439 - acc: 0.2667\n",
      "Epoch 48/1000\n",
      "75/75 [==============================] - 0s 254us/sample - loss: 0.2424 - acc: 0.2667\n",
      "Epoch 49/1000\n",
      "75/75 [==============================] - 0s 270us/sample - loss: 0.2395 - acc: 0.2667\n",
      "Epoch 50/1000\n",
      "75/75 [==============================] - 0s 240us/sample - loss: 0.2375 - acc: 0.2667\n",
      "Epoch 51/1000\n",
      "75/75 [==============================] - 0s 221us/sample - loss: 0.2348 - acc: 0.2667\n",
      "Epoch 52/1000\n",
      "75/75 [==============================] - 0s 243us/sample - loss: 0.2324 - acc: 0.2667\n",
      "Epoch 53/1000\n",
      "75/75 [==============================] - 0s 207us/sample - loss: 0.2305 - acc: 0.2667\n",
      "Epoch 54/1000\n",
      "75/75 [==============================] - 0s 222us/sample - loss: 0.2287 - acc: 0.2667\n",
      "Epoch 55/1000\n",
      "75/75 [==============================] - 0s 205us/sample - loss: 0.2278 - acc: 0.2667\n",
      "Epoch 56/1000\n",
      "75/75 [==============================] - 0s 247us/sample - loss: 0.2261 - acc: 0.2667\n",
      "Epoch 57/1000\n",
      "75/75 [==============================] - 0s 202us/sample - loss: 0.2249 - acc: 0.2667\n",
      "Epoch 58/1000\n",
      "75/75 [==============================] - 0s 215us/sample - loss: 0.2231 - acc: 0.2667\n",
      "Epoch 59/1000\n",
      "75/75 [==============================] - 0s 201us/sample - loss: 0.2215 - acc: 0.2667\n",
      "Epoch 60/1000\n",
      "75/75 [==============================] - 0s 191us/sample - loss: 0.2196 - acc: 0.2667\n",
      "Epoch 61/1000\n",
      "75/75 [==============================] - 0s 306us/sample - loss: 0.2184 - acc: 0.2667\n",
      "Epoch 62/1000\n",
      "75/75 [==============================] - 0s 191us/sample - loss: 0.2167 - acc: 0.2667\n",
      "Epoch 63/1000\n",
      "75/75 [==============================] - 0s 248us/sample - loss: 0.2156 - acc: 0.2667\n",
      "Epoch 64/1000\n",
      "75/75 [==============================] - 0s 223us/sample - loss: 0.2140 - acc: 0.2667\n",
      "Epoch 65/1000\n",
      "75/75 [==============================] - 0s 214us/sample - loss: 0.2124 - acc: 0.2667\n",
      "Epoch 66/1000\n",
      "75/75 [==============================] - 0s 195us/sample - loss: 0.2120 - acc: 0.2667\n",
      "Epoch 67/1000\n",
      "75/75 [==============================] - 0s 237us/sample - loss: 0.2107 - acc: 0.2667\n",
      "Epoch 68/1000\n",
      "75/75 [==============================] - 0s 263us/sample - loss: 0.2085 - acc: 0.2667\n",
      "Epoch 69/1000\n",
      "75/75 [==============================] - 0s 208us/sample - loss: 0.2085 - acc: 0.2667\n",
      "Epoch 70/1000\n",
      "75/75 [==============================] - 0s 248us/sample - loss: 0.2067 - acc: 0.2667\n",
      "Epoch 71/1000\n",
      "75/75 [==============================] - 0s 292us/sample - loss: 0.2064 - acc: 0.2667\n",
      "Epoch 72/1000\n",
      "75/75 [==============================] - 0s 183us/sample - loss: 0.2054 - acc: 0.2667\n",
      "Epoch 73/1000\n",
      "75/75 [==============================] - 0s 237us/sample - loss: 0.2045 - acc: 0.2667\n",
      "Epoch 74/1000\n",
      "75/75 [==============================] - 0s 227us/sample - loss: 0.2038 - acc: 0.2667\n",
      "Epoch 75/1000\n",
      "75/75 [==============================] - 0s 244us/sample - loss: 0.2038 - acc: 0.2667\n",
      "Epoch 76/1000\n",
      "75/75 [==============================] - 0s 222us/sample - loss: 0.2030 - acc: 0.2667\n",
      "Epoch 77/1000\n",
      "75/75 [==============================] - 0s 208us/sample - loss: 0.2024 - acc: 0.2667\n",
      "Epoch 78/1000\n",
      "75/75 [==============================] - 0s 201us/sample - loss: 0.2025 - acc: 0.2667\n",
      "Epoch 79/1000\n",
      "75/75 [==============================] - 0s 241us/sample - loss: 0.2011 - acc: 0.2667\n",
      "Epoch 80/1000\n",
      "75/75 [==============================] - 0s 204us/sample - loss: 0.1999 - acc: 0.2667\n",
      "Epoch 81/1000\n",
      "75/75 [==============================] - 0s 244us/sample - loss: 0.1998 - acc: 0.2667\n",
      "Epoch 82/1000\n",
      "75/75 [==============================] - 0s 208us/sample - loss: 0.1993 - acc: 0.2667\n",
      "Epoch 83/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 234us/sample - loss: 0.1981 - acc: 0.2667\n",
      "Epoch 84/1000\n",
      "75/75 [==============================] - 0s 204us/sample - loss: 0.1978 - acc: 0.2667\n",
      "Epoch 85/1000\n",
      "75/75 [==============================] - 0s 222us/sample - loss: 0.1973 - acc: 0.2667\n",
      "Epoch 86/1000\n",
      "75/75 [==============================] - 0s 188us/sample - loss: 0.1968 - acc: 0.2667\n",
      "Epoch 87/1000\n",
      "75/75 [==============================] - 0s 209us/sample - loss: 0.1959 - acc: 0.2667\n",
      "Epoch 88/1000\n",
      "75/75 [==============================] - 0s 185us/sample - loss: 0.1953 - acc: 0.2667\n",
      "Epoch 89/1000\n",
      "75/75 [==============================] - 0s 197us/sample - loss: 0.1947 - acc: 0.2667\n",
      "Epoch 90/1000\n",
      "75/75 [==============================] - 0s 207us/sample - loss: 0.1942 - acc: 0.2667\n",
      "Epoch 91/1000\n",
      "75/75 [==============================] - 0s 195us/sample - loss: 0.1932 - acc: 0.2667\n",
      "Epoch 92/1000\n",
      "75/75 [==============================] - 0s 201us/sample - loss: 0.1940 - acc: 0.2667\n",
      "Epoch 93/1000\n",
      "75/75 [==============================] - 0s 192us/sample - loss: 0.1928 - acc: 0.2667\n",
      "Epoch 94/1000\n",
      "75/75 [==============================] - 0s 182us/sample - loss: 0.1919 - acc: 0.2667\n",
      "Epoch 95/1000\n",
      "75/75 [==============================] - 0s 174us/sample - loss: 0.1915 - acc: 0.2667\n",
      "Epoch 96/1000\n",
      "75/75 [==============================] - 0s 179us/sample - loss: 0.1912 - acc: 0.2667\n",
      "Epoch 97/1000\n",
      "75/75 [==============================] - 0s 199us/sample - loss: 0.1909 - acc: 0.2667\n",
      "Epoch 98/1000\n",
      "75/75 [==============================] - 0s 187us/sample - loss: 0.1903 - acc: 0.2667\n",
      "Epoch 99/1000\n",
      "75/75 [==============================] - 0s 202us/sample - loss: 0.1902 - acc: 0.2667\n",
      "Epoch 100/1000\n",
      "75/75 [==============================] - 0s 216us/sample - loss: 0.1898 - acc: 0.2667\n",
      "Epoch 101/1000\n",
      "75/75 [==============================] - 0s 206us/sample - loss: 0.1884 - acc: 0.2667\n",
      "Epoch 102/1000\n",
      "75/75 [==============================] - 0s 183us/sample - loss: 0.1879 - acc: 0.2667\n",
      "Epoch 103/1000\n",
      "75/75 [==============================] - 0s 223us/sample - loss: 0.1883 - acc: 0.2667\n",
      "Epoch 104/1000\n",
      "75/75 [==============================] - 0s 188us/sample - loss: 0.1876 - acc: 0.2667\n",
      "Epoch 105/1000\n",
      "75/75 [==============================] - 0s 264us/sample - loss: 0.1869 - acc: 0.2667\n",
      "Epoch 106/1000\n",
      "75/75 [==============================] - 0s 238us/sample - loss: 0.1864 - acc: 0.2667\n",
      "Epoch 107/1000\n",
      "75/75 [==============================] - 0s 190us/sample - loss: 0.1862 - acc: 0.2667\n",
      "Epoch 108/1000\n",
      "75/75 [==============================] - 0s 215us/sample - loss: 0.1859 - acc: 0.2667\n",
      "Epoch 109/1000\n",
      "75/75 [==============================] - 0s 193us/sample - loss: 0.1858 - acc: 0.2667\n",
      "Epoch 110/1000\n",
      "75/75 [==============================] - 0s 260us/sample - loss: 0.1857 - acc: 0.2667\n",
      "Epoch 111/1000\n",
      "75/75 [==============================] - 0s 214us/sample - loss: 0.1857 - acc: 0.2667\n",
      "Epoch 112/1000\n",
      "75/75 [==============================] - 0s 223us/sample - loss: 0.1854 - acc: 0.2667\n",
      "Epoch 113/1000\n",
      "75/75 [==============================] - 0s 241us/sample - loss: 0.1847 - acc: 0.2667\n",
      "Epoch 114/1000\n",
      "75/75 [==============================] - 0s 192us/sample - loss: 0.1844 - acc: 0.2667\n",
      "Epoch 115/1000\n",
      "75/75 [==============================] - 0s 212us/sample - loss: 0.1843 - acc: 0.2667\n",
      "Epoch 116/1000\n",
      "75/75 [==============================] - 0s 215us/sample - loss: 0.1838 - acc: 0.2667\n",
      "Epoch 117/1000\n",
      "75/75 [==============================] - 0s 220us/sample - loss: 0.1836 - acc: 0.2667\n",
      "Epoch 118/1000\n",
      "75/75 [==============================] - 0s 186us/sample - loss: 0.1837 - acc: 0.2667\n",
      "Epoch 119/1000\n",
      "75/75 [==============================] - 0s 209us/sample - loss: 0.1830 - acc: 0.2667\n",
      "Epoch 120/1000\n",
      "75/75 [==============================] - 0s 186us/sample - loss: 0.1825 - acc: 0.2667\n",
      "Epoch 121/1000\n",
      "75/75 [==============================] - 0s 210us/sample - loss: 0.1822 - acc: 0.2667\n",
      "Epoch 122/1000\n",
      "75/75 [==============================] - 0s 206us/sample - loss: 0.1821 - acc: 0.2667\n",
      "Epoch 123/1000\n",
      "75/75 [==============================] - 0s 198us/sample - loss: 0.1821 - acc: 0.2667\n",
      "Epoch 124/1000\n",
      "75/75 [==============================] - 0s 190us/sample - loss: 0.1814 - acc: 0.2667\n",
      "Epoch 125/1000\n",
      "75/75 [==============================] - 0s 189us/sample - loss: 0.1813 - acc: 0.2667\n",
      "Epoch 126/1000\n",
      "75/75 [==============================] - 0s 179us/sample - loss: 0.1813 - acc: 0.2667\n",
      "Epoch 127/1000\n",
      "75/75 [==============================] - 0s 169us/sample - loss: 0.1813 - acc: 0.2667\n",
      "Epoch 128/1000\n",
      "75/75 [==============================] - 0s 220us/sample - loss: 0.1809 - acc: 0.2667\n",
      "Epoch 129/1000\n",
      "75/75 [==============================] - 0s 186us/sample - loss: 0.1808 - acc: 0.2667\n",
      "Epoch 130/1000\n",
      "75/75 [==============================] - 0s 203us/sample - loss: 0.1809 - acc: 0.2667\n",
      "Epoch 131/1000\n",
      "75/75 [==============================] - 0s 229us/sample - loss: 0.1797 - acc: 0.2667\n",
      "Epoch 132/1000\n",
      "75/75 [==============================] - 0s 190us/sample - loss: 0.1800 - acc: 0.2667\n",
      "Epoch 133/1000\n",
      "75/75 [==============================] - 0s 190us/sample - loss: 0.1794 - acc: 0.2667\n",
      "Epoch 134/1000\n",
      "10/75 [===>..........................] - ETA: 0s - loss: 0.5768 - acc: 0.2000"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "Ann = Sequential()\n",
    "Ann.add(Dense(6, input_dim=4, kernel_initializer='he_uniform', activation='softmax'))\n",
    "Ann.add(Dense(1, activation = 'sigmoid'))\n",
    "Ann.compile(optimizer = 'sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "Ann.fit(x_train, y_train, batch_size = 10, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9acc0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction = Ann.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda648db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "mat = mean_squared_error(prediction, y_test)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86912b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54743a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
